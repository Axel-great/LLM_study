{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2的手写代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bb8bd00e30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 设置随机数种子，保证随机数相同\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPT2config():\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 64\n",
    "    n_layer : int = 8\n",
    "    n_head : int = 12 \n",
    "    n_embd: int = 768 # 这个和hidden_dim一样\n",
    "    head_size : int = n_head // n_head\n",
    "    dropout : int = 0.1\n",
    "    vocab_size : int = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, config)->None:\n",
    "        super().__init__()\n",
    "        self.head_size = config.head_size\n",
    "        self.q_proj = nn.Linear(config.n_embd,config.head_size)\n",
    "        self.k_proj = nn.Linear(config.n_embd,config.head_size)\n",
    "        self.v_proj = nn.Linear(config.n_embd,config.head_size)\n",
    "\n",
    "        #注册器创建mask，不会梯度回传，减少计算量\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            torch.tril(torch.ones(config.block_size,config.block_size))\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)       \n",
    "\n",
    "    def forward(self,x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        # (batch,seq,head_dim)\n",
    "\n",
    "        middle_value = q @ k.transpose(-2,-1)\n",
    "        weight = middle_value.masked_fill(\n",
    "            self.attention_mask == 0,\n",
    "            float('-inf'),\n",
    "            ) / math.sqrt(self.head_size) \n",
    "        weight = self.dropout(torch.softmax(weight,dim=-1)) @ v\n",
    "        return final\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SingleHeadAttention(config)\n",
    "            for _ in range(config.n_head)\n",
    "        ])\n",
    "        # 做一次空间变换\n",
    "        self.linear_proj = nn.Linear(config.n_embd,config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self,x):\n",
    "        # x -> (batch_size,seq,head_size) -> (batch_size,seq,n_head * head_size)\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in range(self.blocks)],dim=-1\n",
    "        )\n",
    "        output = self.linear_proj(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd,4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd,config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.ffn(x)\n",
    "# 把多头和ffn合并成一个大的block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        self.muti_head = MultiHeadAttention(config)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x+self.layer_norm(self.muti_head(x))\n",
    "        x = x+self.layer_norm(self.ffn(x))\n",
    "        return x\n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # x -> (batch,seq)  to -> (batch,seq,n_embedding_dim) 在此表vocabulary_size 中找到对应的n_embd个特征\n",
    "        self.word_embedding = nn.Embedding(config.vocab_size,config.n_embd)\n",
    "        self.position_embedding = nn.Embedding(config.block_size,config.n_embd)\n",
    "        # 定义有多少个block 块运行\n",
    "        self.Block = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        # 转化为最终的预测\n",
    "        # 如果有weight tie 需要关掉linear的bias更新，因为Embedding中没有bias\n",
    "        self.linear_proj = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self,module):\n",
    "            # 正态分布初始化\n",
    "            # isinstance 识别是否为Linear层，用于判断\n",
    "            if isinstance(module,nn.Linear):\n",
    "                torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n",
    "                if module.bias is not None: #有bias情况下\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module,nn.Embedding):\n",
    "                torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n",
    "        \n",
    "    def forward(self,x,targets = 0):\n",
    "\n",
    "        # 要确保 词信息和位置信息长度相同\n",
    "        tokens_word = self.word_embedding(x)\n",
    "        tokens_pos = self.position_embedding(torch.arange(seq,device=x.device))\n",
    "        x = tokens_word + tokens_pos\n",
    "        \n",
    "        #送到中间的block当中\n",
    "        x = self.Block(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_proj(x)\n",
    "        x = torch.softmax(x)\n",
    "        batch,seq,vocab_size = x.size()\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # final -> (batch,seq,vocab_size)\n",
    "            # 做预测需要变为(batch * seq,n_embd)\n",
    "            # 这里会将最后所有的tokens转化为 词典表里面每一个词典的概率，查看target的值是否为vocab_size的最大值，最后计算其交叉熵值，最后不断梯度下降更新这个loss\n",
    "            x = x.view(batch * seq ,vocab_size)\n",
    "\n",
    "            # target -> (batch,seq)\n",
    "            # 做预测需要变为 (batch * seq)\n",
    "            target = target.view(batch * seq)\n",
    "\n",
    "            # 做交叉熵计算\n",
    "            # 自带softmax\n",
    "            loss = F.cross_entropy(x,target=target)\n",
    "            return x,loss\n",
    "    \n",
    "    def generate(self,): #todo\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集整理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,path,block_size = 512):\n",
    "        super().__init__()\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        # 定义长度大小\n",
    "        self.block_size = block_size\n",
    "        # 结束符 允许这个编码值被输出，如果不加allowed会被视为普通的tokens\n",
    "        # 如果没有加allow，就没有办法被作为分割符号，并且输出的值也不会是加了allow的token，因为它会进行字符的拆分，再进行编码，和每个不同编码效果相关\n",
    "        self.end_tokens = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        # 用于存放最终用分割符切片好的，带有eos的block_size大小的数组\n",
    "        self.encode_data = []\n",
    "\n",
    "\n",
    "        import json\n",
    "        # 首先读取文本的前1000条\n",
    "        self.max_line = 1000\n",
    "        # 临时存放数据\n",
    "        raw_data = []\n",
    "\n",
    "        with open(path,'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >=self.max_line:\n",
    "                    break\n",
    "                try:\n",
    "                    # 读取每一行text后的文本，并且去掉空格\n",
    "                    text = json.load(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                # 所有异常都跳过不报错\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # 为读取到的每个数据增加分隔符\n",
    "        full_data = []\n",
    "        for text in raw_data:\n",
    "            data = self.encode_data(text)\n",
    "            # 这里用extend是为了把所有句子都连起来，用分隔符号划分句子\n",
    "            full_data.extend(data + [self.end_tokens])\n",
    "        \n",
    "        # 训练时，可以每一个block_size为长度进行训练，因为有分割符号eos 模型不会跨句子学习信息\n",
    "        \n",
    "        for i in range(0,len(full_data),block_size):\n",
    "            # 如果超过边界，则会进行切片操作，python不会报错\n",
    "            chunk = full_data[i:i+block_size+1] # 偏移一位，用于预测 inputs 和label不应该相等\n",
    "            # 如果在最后长度不够，则使用eos_tokens填充\n",
    "            if len(chunk) < block_size + 1 :\n",
    "                chunk = chunk + [self.end_tokens] * (block_size + 1 - len(chunk))\n",
    "            self.encode_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encode_data)\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        chunk = self.encode_data(x)\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)  # 去掉最后一个字符作为输入 \n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)   # 去掉最后一个字符作为输出\n",
    "        return x,y\n",
    "    #  GPT编码与解码\n",
    "    def encode(self,x):\n",
    "        return self.enc.encode(x)\n",
    "    \n",
    "    def decode(self,x):\n",
    "        return self.enc.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\deepseek\\\\deep-learning\\\\mobvoi_seq_monkey_general_open_corpus.jsonl.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[43mMyDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mE:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdeepseek\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mdeep-learning\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmobvoi_seq_monkey_general_open_corpus.jsonl.tar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m train_data,val_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(train_data,[\u001b[38;5;241m0.9\u001b[39m,\u001b[38;5;241m0.1\u001b[39m])\n\u001b[0;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \u001b[38;5;66;03m# shuffle 每次迭代时打乱数据\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m, in \u001b[0;36mMyDataset.__init__\u001b[1;34m(self, path, block_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 临时存放数据\u001b[39;00m\n\u001b[0;32m     23\u001b[0m raw_data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_line:\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\pytorch\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:\\\\deepseek\\\\deep-learning\\\\mobvoi_seq_monkey_general_open_corpus.jsonl.tar'"
     ]
    }
   ],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "\n",
    "\n",
    "datasets = MyDataset('E:\\\\deepseek\\\\deep-learning\\\\mobvoi_seq_monkey_general_open_corpus.jsonl.tar')\n",
    "\n",
    "train_data,val_data = torch.utils.data.random_split(train_data,[0.9,0.1])\n",
    "\n",
    "train_loader = DataLoader(train_data,batch_size=12,shuffle=True)    # shuffle 每次迭代时打乱数据\n",
    "val_loader = DataLoader(val_data,batch_size=12,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wangx\\AppData\\Local\\Temp\\ipykernel_23396\\2704018163.py:105: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n",
      "C:\\Users\\wangx\\AppData\\Local\\Temp\\ipykernel_23396\\2704018163.py:101: FutureWarning: `nn.init.normal` is now deprecated in favor of `nn.init.normal_`.\n",
      "  torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n"
     ]
    }
   ],
   "source": [
    "model = GPT2(GPT2config)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "# 优化器\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=3e-4)\n",
    "# 余弦退火算法，1000次以后学习率为最小值\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练函数\n",
    "def train(model,optimizer,scheduler,train_loader,val_loader,device,epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    # 在这里面，dataloader会自动调用Mydataset中的__get__item方法，获取x、y 所以在继承MyDataset时，需要完成这个方法\n",
    "    for idx,(x,y) in enumerate(train_loader):\n",
    "        x,y = x.to(device),y.to(device)\n",
    "\n",
    "            # 前向传播,计算loss\n",
    "        logits,loss = model(x,target=y)\n",
    "\n",
    "\n",
    "        # 大批量下，需要清除上次的梯度，否则pytorch会自动累加\n",
    "        # 在小批量下可以每几次清除一次梯度，达到小cpu下，运行大批量的效果\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "\n",
    "        # 优化器更新权重值\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 余弦退火算法更新学习率\n",
    "        scheduler.step()\n",
    "\n",
    "        # 损失值累加\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # 每100\n",
    "        if idx % 100 == 0:\n",
    "            print(f'Epoch:{epoch},Batch:{idx},Loss:{loss.item():.4f}')\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "def eval(model,val_loader,device):\n",
    "     # 验证\n",
    "     val_loss = 0\n",
    "     with torch.no_grad():\n",
    "        for x,y in val_loader:\n",
    "                x,y = x.to(device),y.to(device)\n",
    "                logits,loss = model(x,target=y)\n",
    "                val_loss += loss.item()\n",
    "     return val_loss\n",
    "     \n",
    "for epoch in range(2):\n",
    "     train_loss = train(model,optimizer,scheduler,train_loader,val_loader,device)\n",
    "     val_loss = eval(model,val_loader,device)\n",
    "     print(f'Epoch: {epoch}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "     avg_val_loss = val_loss / len(val_loader)\n",
    "     checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'val_loss': avg_val_loss,\n",
    "     }\n",
    "     # 保存每个epoch的模型 参数状态、优化器状态、学习算法状态\n",
    "     torch.save(checkpoint, f'checkpoints/model_epoch_{epoch}.pt')\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 计算 <|endoftext|> 的 token ID\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m eos_token \u001b[38;5;241m=\u001b[39m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd-of-text token ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, eos_token)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\pytorch\\lib\\site-packages\\tiktoken\\core.py:117\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[38;5;241m.\u001b[39msearch(text):\n\u001b[1;32m--> 117\u001b[0m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(allowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\pytorch\\lib\\site-packages\\tiktoken\\core.py:400\u001b[0m, in \u001b[0;36mraise_disallowed_special_token\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as a special token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 获取 GPT-2 的编码器\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 计算 <|endoftext|> 的 token ID\n",
    "eos_token = enc.encode(\"<|endoftext|>\")\n",
    "\n",
    "print(\"End-of-text token ID:\", eos_token)  # 输出: 50256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
