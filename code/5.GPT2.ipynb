{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPT2的手写代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1d51c523370>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import dataloader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# 设置随机数种子，保证随机数相同\n",
    "torch.manual_seed(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GPTconfig():\n",
    "    block_size: int = 512\n",
    "    batch_size: int = 64\n",
    "    n_layer : int = 8\n",
    "    n_head : int = 12 \n",
    "    n_embd: int = 768 # 这个和hidden_dim一样\n",
    "    head_size : int = n_head // n_head\n",
    "    dropout : int = 0.1\n",
    "    vocab_size : int = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型的结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, config)->None:\n",
    "        super().__init__()\n",
    "        self.head_size = config.n_head_size\n",
    "        self.q_proj = nn.Linear(config.n_embd,config.n_head_size)\n",
    "        self.k_proj = nn.Linear(config.n_embd,config.n_head_size)\n",
    "        self.v_proj = nn.Linear(config.n_embd,config.n_head_size)\n",
    "\n",
    "        #注册器创建mask，不会梯度回传，减少计算量\n",
    "        self.register_buffer(\n",
    "            'attention_mask',\n",
    "            torch.tril(config.block_size,config.block_size)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)       \n",
    "\n",
    "    def forward(self,x):\n",
    "        q = self.q_proj(x)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "        # (batch,seq,head_dim)\n",
    "\n",
    "        middle_value = q @ k.transpose(-2,-1)\n",
    "        weight = middle_value.masked_fill(\n",
    "            self.attention_mask == 0,\n",
    "            float('-inf'),\n",
    "            ) / math.sqrt(self.head_size) \n",
    "        weight = self.dropout(torch.softmax(weight,dim=-1)) @ v\n",
    "        return final\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList([\n",
    "            SingleHeadAttention(config)\n",
    "            for _ in range(config.n_head)\n",
    "        ])\n",
    "        # 做一次空间变换\n",
    "        self.linear_proj = nn.Linear(config.n_embd,config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self,x):\n",
    "        # x -> (batch_size,seq,head_size) -> (batch_size,seq,n_head * head_size)\n",
    "        output = torch.cat(\n",
    "            [h(x) for h in range(self.blocks)],dim=-1\n",
    "        )\n",
    "        output = self.linear_proj(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.n_embd,4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd,config.n_embd),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.ffn(x)\n",
    "# 把多头和ffn合并成一个大的block\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.ffn = FeedForward(config)\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        self.muti_head = MultiHeadAttention(config)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = x+self.layer_norm(self.muti_head(x))\n",
    "        x = x+self.layer_norm(self.ffn(x))\n",
    "        return x\n",
    "    \n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "        # x -> (batch,seq)  to -> (batch,seq,n_embedding_dim) 在此表vocabulary_size 中找到对应的n_embd个特征\n",
    "        self.word_embedding = nn.Embedding(config.vocab_size,config.n_embd)\n",
    "        self.position_embedding = nn.Embedding(config.block_size,config.n_embd)\n",
    "        # 定义有多少个block 块运行\n",
    "        self.Block = nn.Sequential(\n",
    "            *[Block(config) for _ in range(config.n_layer)]\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(config.n_embd)\n",
    "        # 转化为最终的预测\n",
    "        # 如果有weight tie 需要关掉linear的bias更新，因为Embedding中没有bias\n",
    "        self.linear_proj = nn.Linear(config.n_embd,config.vocab_size,bias=False)\n",
    "\n",
    "        self.apply(_init_weight)\n",
    "\n",
    "    def _init_weight(self,module):\n",
    "            # 正态分布初始化\n",
    "            # isinstance 识别是否为Linear层，用于判断\n",
    "            if isinstance(module,nn.Linear):\n",
    "                torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n",
    "                if module.bias is not None: #有bias情况下\n",
    "                    torch.nn.init.zeros_(module.bias)\n",
    "            elif isinstance(module,nn.Embedding):\n",
    "                torch.nn.init.normal(module.weight,mean=0.0,std=0.02)\n",
    "        \n",
    "    def forward(self,x,target = 0):\n",
    "\n",
    "        # 要确保 词信息和位置信息长度相同\n",
    "        tokens_word = self.word_embedding(x)\n",
    "        tokens_pos = self.position_embedding(torch.arange(seq,device=x.device))\n",
    "        x = tokens_word + tokens_pos\n",
    "        \n",
    "        #送到中间的block当中\n",
    "        x = self.Block(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.linear_proj(x)\n",
    "        x = torch.softmax(x)\n",
    "        batch,seq,vocab_size = x.size()\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            # final -> (batch,seq,vocab_size)\n",
    "            # 做预测需要变为(batch * seq,n_embd)\n",
    "            # 这里会将最后所有的tokens转化为 词典表里面每一个词典的概率，查看target的值是否为vocab_size的最大值，最后计算其交叉熵值，最后不断梯度下降更新这个loss\n",
    "            x = x.view(batch * seq ,vocab_size)\n",
    "\n",
    "            # target -> (batch,seq)\n",
    "            # 做预测需要变为 (batch * seq)\n",
    "            target = target.view(batch * seq)\n",
    "\n",
    "            # 做交叉熵计算\n",
    "            # 自带softmax\n",
    "            loss = F.cross_entropy(x,target=target)\n",
    "            return x,loss\n",
    "    \n",
    "    def generate(self,): #todo\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集整理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,path,block_size = 512):\n",
    "        super().__init__()\n",
    "        import tiktoken\n",
    "        self.enc = tiktoken.get_encoding(\"gpt2\")\n",
    "        # 定义长度大小\n",
    "        self.block_size = block_size\n",
    "        # 结束符 允许这个编码值被输出，如果不加allowed会被视为普通的tokens\n",
    "        # 如果没有加allow，就没有办法被作为分割符号，并且输出的值也不会是加了allow的token，因为它会进行字符的拆分，再进行编码，和每个不同编码效果相关\n",
    "        self.end_tokens = self.enc.encode(\n",
    "            \"<|endoftext|>\",\n",
    "            allowed_special={\"<|endoftext|>\"}\n",
    "        )[0]\n",
    "\n",
    "        # 用于存放最终用分割符切片好的，带有eos的block_size大小的数组\n",
    "        self.encode_data = []\n",
    "\n",
    "\n",
    "        import json\n",
    "        # 首先读取文本的前1000条\n",
    "        self.max_line = 1000\n",
    "        # 临时存放数据\n",
    "        raw_data = []\n",
    "\n",
    "        with open(path,'r') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >=self.max_line:\n",
    "                    break\n",
    "                try:\n",
    "                    # 读取每一行text后的文本，并且去掉空格\n",
    "                    text = json.load(line.strip())['text']\n",
    "                    raw_data.append(text)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                # 所有异常都跳过不报错\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        # 为读取到的每个数据增加分隔符\n",
    "        full_data = []\n",
    "        for text in raw_data:\n",
    "            data = self.encode_data(text)\n",
    "            # 这里用extend是为了把所有句子都连起来，用分隔符号划分句子\n",
    "            full_data.extend(data + [self.end_tokens])\n",
    "        \n",
    "        # 训练时，可以每一个block_size为长度进行训练，因为有分割符号eos 模型不会跨句子学习信息\n",
    "        \n",
    "        for i in range(0,len(full_data),block_size):\n",
    "            # 如果超过边界，则会进行切片操作，python不会报错\n",
    "            chunk = full_data[i:i+block_size+1] # 偏移一位，用于预测 inputs 和label不应该相等\n",
    "            # 如果在最后长度不够，则使用eos_tokens填充\n",
    "            if chunk < block_size + 1 :\n",
    "                chunk = chunk + [self.end_tokens] * (block_size + 1 - len(chunk))\n",
    "            self.encode_data.append(chunk)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encode_data)\n",
    "    \n",
    "    def __getitem__(self,x):\n",
    "        chunk = self.encode_data(x)\n",
    "        x = torch.tensor(chunk[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(chunk[1:], dtype=torch.long)\n",
    "        return x,y\n",
    "    #  GPT编码与解码\n",
    "    def encode(self,x):\n",
    "        return self.enc.encode(x)\n",
    "    \n",
    "    def decode(self,x):\n",
    "        return self.enc.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m enc \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 计算 <|endoftext|> 的 token ID\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m eos_token \u001b[38;5;241m=\u001b[39m \u001b[43menc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|endoftext|>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd-of-text token ID:\u001b[39m\u001b[38;5;124m\"\u001b[39m, eos_token)\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\pytorch\\lib\\site-packages\\tiktoken\\core.py:117\u001b[0m, in \u001b[0;36mEncoding.encode\u001b[1;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[0;32m    115\u001b[0m         disallowed_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m(disallowed_special)\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;241m:=\u001b[39m _special_token_regex(disallowed_special)\u001b[38;5;241m.\u001b[39msearch(text):\n\u001b[1;32m--> 117\u001b[0m         \u001b[43mraise_disallowed_special_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# https://github.com/PyO3/pyo3/pull/3632\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(allowed_special, \u001b[38;5;28mfrozenset\u001b[39m):\n",
      "File \u001b[1;32md:\\apps\\anaconda\\envs\\pytorch\\lib\\site-packages\\tiktoken\\core.py:400\u001b[0m, in \u001b[0;36mraise_disallowed_special_token\u001b[1;34m(token)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_disallowed_special_token\u001b[39m(token: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered text corresponding to disallowed special token \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as a special token, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpass it to `allowed_special`, e.g. `allowed_special=\u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m, ...\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want this text to be encoded as normal text, disable the check for this token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby passing `disallowed_special=(enc.special_tokens_set - \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[38;5;124m)`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo disable this check for all special tokens, pass `disallowed_special=()`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Encountered text corresponding to disallowed special token '<|endoftext|>'.\nIf you want this text to be encoded as a special token, pass it to `allowed_special`, e.g. `allowed_special={'<|endoftext|>', ...}`.\nIf you want this text to be encoded as normal text, disable the check for this token by passing `disallowed_special=(enc.special_tokens_set - {'<|endoftext|>'})`.\nTo disable this check for all special tokens, pass `disallowed_special=()`.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# 获取 GPT-2 的编码器\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# 计算 <|endoftext|> 的 token ID\n",
    "eos_token = enc.encode(\"<|endoftext|>\")\n",
    "\n",
    "print(\"End-of-text token ID:\", eos_token)  # 输出: 50256"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
