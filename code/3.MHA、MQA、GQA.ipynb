{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GQA相比于MHA，是减少了KV的生成，用少量的KV去计算不同Linear下的Q。  \n",
    "而代码的本质是生成少量的KV，然后将其复制与Q等量，进行计算。  \n",
    "也就是要保证kv_head_num可以被head_num整除 \n",
    "\n",
    "MQA就是GQA的kv_head_num =1的特殊情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1449,  0.0688, -0.0461,  ...,  0.0805, -0.3760,  0.0840],\n",
       "         [-0.1447,  0.0678, -0.0487,  ...,  0.0787, -0.3763,  0.0834],\n",
       "         [-0.1440,  0.0697, -0.0493,  ...,  0.0791, -0.3751,  0.0828],\n",
       "         [-0.1427,  0.0668, -0.0483,  ...,  0.0785, -0.3745,  0.0823]],\n",
       "\n",
       "        [[-0.1132,  0.1124,  0.0269,  ...,  0.0236, -0.3805,  0.0164],\n",
       "         [-0.1157,  0.1130,  0.0260,  ...,  0.0229, -0.3821,  0.0152],\n",
       "         [-0.1129,  0.1101,  0.0265,  ...,  0.0241, -0.3835,  0.0160],\n",
       "         [-0.1141,  0.1158,  0.0227,  ...,  0.0215, -0.3825,  0.0189]],\n",
       "\n",
       "        [[-0.0658,  0.1407,  0.0395,  ...,  0.1017, -0.4091,  0.0375],\n",
       "         [-0.0658,  0.1381,  0.0383,  ...,  0.1015, -0.4099,  0.0401],\n",
       "         [-0.0691,  0.1389,  0.0382,  ...,  0.1002, -0.4114,  0.0376],\n",
       "         [-0.0638,  0.1386,  0.0380,  ...,  0.1002, -0.4097,  0.0365]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from  torch import nn\n",
    "from torch import functional as F\n",
    "import math\n",
    "# 这个代码省略了mask和dropout\n",
    "class GruopAttention(nn.Module):\n",
    "    def __init__(self,hidden_dim,head_num,kv_head_num):\n",
    "        super().__init__()\n",
    "        # 保证整除的关系\n",
    "        assert hidden_dim % head_num == 0\n",
    "        assert head_num % kv_head_num ==0\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.kv_head_num = kv_head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "\n",
    "        # Q(batch,seq,hidden_dim) KV(batch,seq,kv_head_num * head_dim) \n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,kv_head_num * self.head_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, kv_head_num * self.head_dim)\n",
    "        # 最终还是要输出相同的格式\n",
    "        self.output = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,X):\n",
    "        batch,seq,_ = X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        # (batch,seq,head_num,head_dim) -> (batch,head_num,seq,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)\n",
    "        \n",
    "        k = K.view(batch,seq,self.kv_head_num,self.head_dim).transpose(1,2)\n",
    "        v = V.view(batch,seq,self.kv_head_num,self.head_dim).transpose(1,2)\n",
    "\n",
    "        # 对KV的维度进行repeat\n",
    "        # (batch,kv_head_num,seq,head_dim) -> (batch,head_num,seq,head_dim)\n",
    "        # repeat次数为 head_num // kv_head_num\n",
    "        # 在维度1上进行repeat\n",
    "        k = k.repeat_interleave(self.head_num // self.kv_head_num,dim = 1)\n",
    "        v = v.repeat_interleave(self.head_num // self.kv_head_num,dim  =1)\n",
    "\n",
    "        # 之后就是与MHA相同的操作\n",
    "        # 注意指定softmax的维度\n",
    "        attention_mid = torch.softmax((q @ k.transpose(-1,-2)) / math.sqrt(self.head_num),dim=-1)\n",
    "\n",
    "        #中间的dropout省略\n",
    "\n",
    "        #(batch,head_num,seq,head_dim) -> (batch,seq,head_num * head_dim)\n",
    "        output_mid = ((attention_mid @ v)\n",
    "            .transpose(1,2)\n",
    "            .contiguous())\n",
    "        output_mid = output_mid.view(batch,seq,self.hidden_dim)\n",
    "\n",
    "        output = self.output(output_mid)\n",
    "        return output\n",
    "\n",
    "X = torch.rand(3,4,128)\n",
    "net = GruopAttention(128,8,4)\n",
    "output = net(X)\n",
    "print(output.shape)\n",
    "output\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
