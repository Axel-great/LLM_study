{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1647, -0.5922,  0.2470, -0.0232],\n",
       "         [-0.1650, -0.5883,  0.2439, -0.0200],\n",
       "         [-0.1641, -0.5926,  0.2476, -0.0244]],\n",
       "\n",
       "        [[-0.2515, -0.7480,  0.2592, -0.1423],\n",
       "         [-0.2503, -0.7461,  0.2571, -0.1425],\n",
       "         [-0.2504, -0.7468,  0.2562, -0.1411]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim:int = 728)->None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "    #初始化qkv\n",
    "        self.query_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # Q : (batch_size,seq_len,dim)\n",
    "        # K*T:(batch_size,dim,seq_len)\n",
    "        Q = self.query_proj(X)  # X (batch_size,seq,dim) -> (batch_size,seq,hidden_dim)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        attention_value = torch.matmul(\n",
    "            #给k转置\n",
    "            Q,K.transpose(-1,-2)\n",
    "        )\n",
    "\n",
    "        attention_softmax = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim),dim=-1\n",
    "        )\n",
    "\n",
    "        result = torch.matmul(\n",
    "            attention_softmax,V\n",
    "        )\n",
    "        return result\n",
    "\n",
    "X = torch.rand(2,3,4)\n",
    "\n",
    "\n",
    "self_att_test = selfAttention(4)\n",
    "self_att_test(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4894, 0.5106],\n",
      "         [0.5109, 0.4891]],\n",
      "\n",
      "        [[0.5137, 0.4863],\n",
      "         [0.4978, 0.5022]],\n",
      "\n",
      "        [[0.5048, 0.4952],\n",
      "         [0.5059, 0.4941]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4449,  0.2280, -0.5718,  0.0779],\n",
       "         [-0.4421,  0.2280, -0.5792,  0.0705]],\n",
       "\n",
       "        [[-0.5378,  0.1127, -0.5518,  0.1663],\n",
       "         [-0.5330,  0.1195, -0.5498,  0.1640]],\n",
       "\n",
       "        [[-0.5221,  0.0467, -0.6415,  0.1114],\n",
       "         [-0.5225,  0.0464, -0.6415,  0.1117]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim,dim*3)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # \n",
    "        QKV = self.qkv(X)\n",
    "        Q,K,V = torch.split(QKV,self.dim,dim=-1)\n",
    "\n",
    "        result = torch.softmax(torch.matmul(\n",
    "            Q,K.transpose(-1,-2)\n",
    "        ) / math.sqrt(self.dim),dim=-1\n",
    "        )\n",
    "        print(result)\n",
    "        output = result @ V\n",
    "        return output\n",
    "X = torch.rand(3,2,4)\n",
    "model2 = selfAttentionV2(4)\n",
    "model2(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadTransformer(nn.Module):\n",
    "    def __init__(self,head_num,hidden_dim,drop_out = 0.1):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        \n",
    "        self.out_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,X,hidden_mask = None):\n",
    "        #   qkv (batch,s,h)  ->   (batch,head_num,s,head_dim)\n",
    "        batch , seq , _ =  X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        #   transform it's shape (batch,s,head_num,head_dim)  (batch,head_num,s,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        k = K.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        v = V.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)  \n",
    "\n",
    "        result_mid = (q @ k.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if hidden_mask is not None:\n",
    "            result_mid = result_mid.masked_fill(\n",
    "                hidden_mask == 0 , float('-inf') #   无限小的值\n",
    "            )\n",
    "        print(result_mid)\n",
    "\n",
    "        #   对行向量进行softmax\n",
    "        result_mid = torch.softmax(result_mid,dim=-1)\n",
    "        result_drop = self.drop_out(result_mid)\n",
    "\n",
    "        output_mid = (result_drop @ v).transpose(1,2).contiguous()\n",
    "        output_mid = output_mid.view(batch,seq,-1)#h\n",
    "\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "\n",
    "mask = torch.tensor([\n",
    "    [1,0],\n",
    "    [0,0],\n",
    "    [1,1]\n",
    "]).unsqueeze(1).unsqueeze(2).expand(3,8,2,2)\n",
    "print(mask.shape)\n",
    "X = torch.rand(3,2,128)\n",
    "model = MutiheadTransformer(8,128)# 8 head\n",
    "model(X,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MutiHead 中QKV的不同的头，即 将其拆分为不同的特征向量。他们经过不同的初始化之后有着不同的权重值，再将其**语义的特征向量**进行拆分，并分别与不同的KV矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0299, 0.0615, 0.0786, 0.0514, 0.1575, 0.0586, 0.0886, 0.0536,\n",
       "          0.1314, 0.1477, 0.0744, 0.0667],\n",
       "         [0.0370, 0.0445, 0.0565, 0.0635, 0.1195, 0.0328, 0.1554, 0.0355,\n",
       "          0.0491, 0.2019, 0.0530, 0.1513],\n",
       "         [0.0189, 0.0400, 0.0905, 0.1386, 0.1475, 0.0352, 0.0664, 0.0400,\n",
       "          0.0685, 0.2035, 0.0357, 0.1150],\n",
       "         [0.0210, 0.0402, 0.0771, 0.1360, 0.1415, 0.0348, 0.0512, 0.0334,\n",
       "          0.0797, 0.2284, 0.0266, 0.1299]],\n",
       "\n",
       "        [[0.0567, 0.0522, 0.0610, 0.1105, 0.0988, 0.0443, 0.1244, 0.0548,\n",
       "          0.0397, 0.1198, 0.0902, 0.1478],\n",
       "         [0.0344, 0.0637, 0.1130, 0.1915, 0.1115, 0.0312, 0.0915, 0.0554,\n",
       "          0.0373, 0.0636, 0.0412, 0.1659],\n",
       "         [0.0400, 0.0625, 0.0957, 0.1342, 0.1082, 0.0427, 0.1139, 0.0451,\n",
       "          0.0280, 0.1210, 0.0431, 0.1656],\n",
       "         [0.0524, 0.0748, 0.1068, 0.1486, 0.0640, 0.0352, 0.1018, 0.0440,\n",
       "          0.0347, 0.1116, 0.0392, 0.1869]],\n",
       "\n",
       "        [[0.0500, 0.0579, 0.0581, 0.1085, 0.0823, 0.0383, 0.1333, 0.0549,\n",
       "          0.0353, 0.0950, 0.0994, 0.1870],\n",
       "         [0.0333, 0.0534, 0.0737, 0.1422, 0.0877, 0.0262, 0.1590, 0.0323,\n",
       "          0.0332, 0.1323, 0.0517, 0.1751],\n",
       "         [0.0502, 0.0746, 0.0879, 0.1314, 0.0473, 0.0337, 0.1494, 0.0374,\n",
       "          0.0305, 0.1750, 0.0541, 0.1286],\n",
       "         [0.0641, 0.0492, 0.0462, 0.0898, 0.0986, 0.0511, 0.1310, 0.0529,\n",
       "          0.0294, 0.0992, 0.1026, 0.1859]]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self,hidden_dim,head_num,dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        # upgrade dimension \n",
    "        self.up = nn.Linear(hidden_dim,hidden_dim*4)\n",
    "        self.down = nn.Linear(hidden_dim*4 , hidden_dim)\n",
    "        self.act_proj = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout_proj = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm_proj = nn.LayerNorm(hidden_dim,eps=0.000001)\n",
    "\n",
    "\n",
    "    def attention_layer(self,q,k,v,mask,):\n",
    "        # (batch,seq,head_num,head_dim) -> (batch,head_num,seq,head_dim)\n",
    "        attention_mid = (q @ k.transpose(-2,-1)) / math.sqrt(self.head_dim)\n",
    "        # Itself tril and original mask\n",
    "        if mask is not None:\n",
    "            mask = mask.tril()\n",
    "            attention_mid = attention_mid.masked_fill(\n",
    "                mask == 0 , float('-inf')\n",
    "            )\n",
    "        else:\n",
    "            # make a ones trangles matrix which dimensions same like original \n",
    "            mask = attention_mid.ones_like().tril()\n",
    "            attention_mid = attention_mid.masked_fill(\n",
    "                mask == 0 , float('-inf')\n",
    "            )\n",
    "        # Specify a dimension row\n",
    "        attention_mid = torch.softmax(attention_mid,dim=-1)\n",
    "        # dropout layer\n",
    "        attention_dropout = self.dropout_proj(attention_mid)\n",
    "        #(batch,head_num,seq,head_dim)\n",
    "        output = (attention_dropout @ v)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def ffn(self,X):\n",
    "\n",
    "        up = self.up(X)\n",
    "        up = self.act_proj(up)\n",
    "        down = self.down(up)\n",
    "        down = self.dropout_proj(down)\n",
    "        return self.layerNorm_proj(down + X)\n",
    "\n",
    "    def mha(self,X,mask):\n",
    "        batch,seq,_ = X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "        # (batch,seq,hidden_dim) -> (batch,seq,head_num,head_dim) -> (batch,head_num,seq,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        k = K.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        v = V.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "\n",
    "        attention = self.attention_layer(q,k,v,mask).transpose(1,2).contiguous()\n",
    "        output = attention.view(batch,seq,-1)\n",
    "        return output\n",
    "\n",
    "    def forward(self,X,attention_mask=None):\n",
    "        X = self.mha(X,attention_mask)\n",
    "        X = self.ffn(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [\n",
    "                SimpleDecoder(64,8) # hidden=64,head_num=8\n",
    "            ]\n",
    "        )\n",
    "        self.emb = nn.Embedding(12,64)\n",
    "        self.out = nn.Linear(64,12)\n",
    "\n",
    "    def forward(self,X,mask=None):\n",
    "        X = self.emb(X)\n",
    "        for i , l in enumerate(self.layer_list):\n",
    "            X = l(X,mask)\n",
    "        print(X.shape)\n",
    "        output = self.out(X)\n",
    "        return torch.softmax(output,dim=-1)\n",
    "    \n",
    "x  = torch.randint(low=0,high=12,size=(3,4))\n",
    "#x = torch.rand(3,4,64)\n",
    "net = Decoder()\n",
    "mask = (\n",
    "    torch.tensor([[1,1,1,1],[1,1,0,0],[1,0,0,0]])\n",
    "    .unsqueeze(1)\n",
    "    .unsqueeze(2)\n",
    "    .repeat(1,8,4,1)\n",
    ")\n",
    "#print(mask.shape)\n",
    "net(x,mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
