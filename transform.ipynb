{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1647, -0.5922,  0.2470, -0.0232],\n",
       "         [-0.1650, -0.5883,  0.2439, -0.0200],\n",
       "         [-0.1641, -0.5926,  0.2476, -0.0244]],\n",
       "\n",
       "        [[-0.2515, -0.7480,  0.2592, -0.1423],\n",
       "         [-0.2503, -0.7461,  0.2571, -0.1425],\n",
       "         [-0.2504, -0.7468,  0.2562, -0.1411]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim:int = 728)->None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "    #初始化qkv\n",
    "        self.query_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # Q : (batch_size,seq_len,dim)\n",
    "        # K*T:(batch_size,dim,seq_len)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        attention_value = torch.matmul(\n",
    "            #给k转置\n",
    "            Q,K.transpose(-1,-2)\n",
    "        )\n",
    "\n",
    "        attention_softmax = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim),dim=-1\n",
    "        )\n",
    "\n",
    "        result = torch.matmul(\n",
    "            attention_softmax,V\n",
    "        )\n",
    "        return result\n",
    "\n",
    "X = torch.rand(2,3,4)\n",
    "\n",
    "\n",
    "self_att_test = selfAttention(4)\n",
    "self_att_test(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4894, 0.5106],\n",
      "         [0.5109, 0.4891]],\n",
      "\n",
      "        [[0.5137, 0.4863],\n",
      "         [0.4978, 0.5022]],\n",
      "\n",
      "        [[0.5048, 0.4952],\n",
      "         [0.5059, 0.4941]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4449,  0.2280, -0.5718,  0.0779],\n",
       "         [-0.4421,  0.2280, -0.5792,  0.0705]],\n",
       "\n",
       "        [[-0.5378,  0.1127, -0.5518,  0.1663],\n",
       "         [-0.5330,  0.1195, -0.5498,  0.1640]],\n",
       "\n",
       "        [[-0.5221,  0.0467, -0.6415,  0.1114],\n",
       "         [-0.5225,  0.0464, -0.6415,  0.1117]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim,dim*3)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # \n",
    "        QKV = self.qkv(X)\n",
    "        Q,K,V = torch.split(QKV,self.dim,dim=-1)\n",
    "\n",
    "        result = torch.softmax(torch.matmul(\n",
    "            Q,K.transpose(-1,-2)\n",
    "        ) / math.sqrt(self.dim),dim=-1\n",
    "        )\n",
    "        print(result)\n",
    "        output = result @ V\n",
    "        return output\n",
    "X = torch.rand(3,2,4)\n",
    "model2 = selfAttentionV2(4)\n",
    "model2(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 8, 2, 2])\n",
      "tensor([[[[ 0.1323,    -inf],\n",
      "          [ 0.0970,    -inf]],\n",
      "\n",
      "         [[ 0.0960,    -inf],\n",
      "          [-0.0008,    -inf]],\n",
      "\n",
      "         [[-0.1000,    -inf],\n",
      "          [-0.0680,    -inf]],\n",
      "\n",
      "         [[ 0.1082,    -inf],\n",
      "          [ 0.1479,    -inf]],\n",
      "\n",
      "         [[-0.1186,    -inf],\n",
      "          [-0.1090,    -inf]],\n",
      "\n",
      "         [[ 0.0476,    -inf],\n",
      "          [ 0.0272,    -inf]],\n",
      "\n",
      "         [[-0.0938,    -inf],\n",
      "          [-0.0341,    -inf]],\n",
      "\n",
      "         [[ 0.1974,    -inf],\n",
      "          [ 0.0012,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]],\n",
      "\n",
      "         [[   -inf,    -inf],\n",
      "          [   -inf,    -inf]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1478, -0.0166],\n",
      "          [-0.0172, -0.1485]],\n",
      "\n",
      "         [[ 0.1257,  0.0386],\n",
      "          [ 0.1520,  0.0917]],\n",
      "\n",
      "         [[-0.1125, -0.0302],\n",
      "          [-0.1040,  0.0131]],\n",
      "\n",
      "         [[ 0.0864,  0.1365],\n",
      "          [ 0.0741,  0.0806]],\n",
      "\n",
      "         [[-0.1716, -0.2043],\n",
      "          [-0.2009, -0.1456]],\n",
      "\n",
      "         [[ 0.0561,  0.0151],\n",
      "          [ 0.0129, -0.0706]],\n",
      "\n",
      "         [[-0.0150, -0.0038],\n",
      "          [-0.0417, -0.0195]],\n",
      "\n",
      "         [[ 0.0679, -0.0265],\n",
      "          [ 0.1012, -0.0272]]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.0130e-01,  3.0907e-01, -3.5146e-01, -2.6524e-01,  2.9541e-01,\n",
       "           1.6806e-01, -1.6961e-01,  2.8238e-01,  7.8997e-02, -9.3792e-02,\n",
       "          -1.4074e-01, -2.9890e-01,  7.3648e-02,  5.9242e-02, -2.5970e-02,\n",
       "          -1.6604e-01,  1.4730e-01,  3.2386e-01,  3.8600e-02, -1.1986e-01,\n",
       "          -4.4942e-02,  3.5500e-02,  2.6568e-02, -7.6863e-02, -4.7323e-03,\n",
       "          -9.5115e-02,  1.1173e-02,  8.6500e-02, -1.3608e-01,  1.3307e-01,\n",
       "           4.8173e-02,  2.8138e-01, -3.1594e-02, -2.8889e-01, -2.1654e-01,\n",
       "          -1.8872e-01,  1.0044e-01,  1.1752e-01, -1.6978e-01,  7.4238e-03,\n",
       "           7.9879e-02, -1.8393e-01, -3.9519e-02,  1.2685e-01,  3.2053e-01,\n",
       "           1.0059e-01, -1.4045e-01,  5.3156e-02, -5.0888e-01,  7.2849e-03,\n",
       "          -9.4807e-03, -2.1026e-02, -4.0783e-01, -4.0632e-02, -4.2254e-01,\n",
       "          -2.3455e-01,  4.1237e-02, -1.0502e-02,  3.2463e-02, -2.3092e-02,\n",
       "           2.4118e-01,  1.2116e-01, -1.2872e-01, -4.2637e-01,  9.7296e-02,\n",
       "          -2.3868e-01,  4.1308e-02,  2.6615e-01, -2.3296e-01, -1.3170e-01,\n",
       "          -2.6550e-01,  2.4203e-01,  2.5994e-01,  2.3585e-01, -7.2725e-02,\n",
       "          -2.6917e-02, -3.5877e-01, -1.9152e-01,  1.1654e-01,  7.9960e-02,\n",
       "          -3.0666e-01,  1.4540e-01, -2.8950e-01,  1.5690e-01,  2.7001e-01,\n",
       "           1.1758e-01, -9.6474e-02, -1.1506e-01, -6.5331e-02, -2.9581e-01,\n",
       "          -6.0837e-02, -8.9368e-02, -1.5523e-01, -4.2225e-01,  9.7386e-02,\n",
       "           6.5028e-03,  2.8996e-01,  2.4591e-04,  1.9776e-01, -2.2919e-01,\n",
       "          -2.2731e-01, -1.3432e-01,  1.9392e-03,  1.2073e-01,  1.3870e-01,\n",
       "          -1.5618e-01, -3.5672e-01,  2.1039e-01,  5.1382e-02, -9.8511e-02,\n",
       "           9.0464e-02, -4.6948e-02,  3.1962e-02, -1.8661e-01,  1.8795e-02,\n",
       "          -3.9815e-01, -3.0093e-01, -9.1114e-02,  1.6400e-01, -1.6942e-01,\n",
       "           6.3744e-02,  4.0575e-01, -5.1187e-02,  5.3052e-01, -3.3157e-01,\n",
       "           2.4279e-01, -1.3150e-01, -7.5855e-02],\n",
       "         [-2.7005e-01,  2.8888e-01, -3.6827e-01, -2.5988e-01,  2.9390e-01,\n",
       "           2.1320e-01, -1.6661e-01,  3.2125e-01,  1.3882e-01, -1.1180e-01,\n",
       "          -1.3564e-01, -3.4083e-01,  6.5734e-02,  9.7038e-02,  6.9615e-02,\n",
       "          -9.5796e-02,  1.4176e-01,  3.7696e-01,  1.2150e-02, -5.7358e-02,\n",
       "          -1.0371e-01,  2.3472e-02,  3.9198e-02, -3.7583e-03, -2.0433e-02,\n",
       "          -7.0577e-02,  3.4682e-02,  1.1356e-01, -1.8016e-01,  1.4823e-01,\n",
       "           6.1676e-02,  3.3915e-01,  5.6568e-02, -2.5745e-01, -3.2468e-01,\n",
       "          -2.1418e-01,  8.2365e-02,  3.0215e-02, -1.7175e-01, -6.1713e-02,\n",
       "           7.9387e-02, -2.1067e-01, -9.8216e-02,  9.8450e-02,  3.7903e-01,\n",
       "           1.1408e-01, -2.5821e-01,  9.5227e-02, -5.9059e-01,  1.6404e-02,\n",
       "          -2.5866e-02, -9.0395e-02, -3.7721e-01,  9.3585e-03, -3.7703e-01,\n",
       "          -2.1061e-01,  1.0856e-01, -8.5865e-02,  4.1636e-03, -5.7910e-02,\n",
       "           2.9172e-01,  7.0593e-02, -2.1136e-01, -5.2205e-01,  2.9047e-02,\n",
       "          -2.9164e-01, -2.6533e-02,  3.6258e-01, -1.8202e-01, -5.0738e-02,\n",
       "          -3.3469e-01,  3.5504e-01,  3.7173e-01,  1.8044e-01, -1.4635e-01,\n",
       "          -3.2912e-02, -3.6817e-01, -2.0631e-01,  1.6039e-01,  1.3061e-01,\n",
       "          -2.9374e-01,  1.8946e-01, -2.6527e-01,  2.7565e-01,  2.9488e-01,\n",
       "           8.4291e-02, -1.0166e-01, -5.5850e-02,  6.7191e-02, -3.1336e-01,\n",
       "          -9.3410e-03, -5.5021e-02, -5.8516e-02, -5.1186e-01,  1.0633e-01,\n",
       "           7.0364e-02,  2.7415e-01, -7.2083e-02,  2.0561e-01, -2.4824e-01,\n",
       "          -1.8300e-01, -1.5230e-01, -1.9937e-03,  1.2588e-01,  1.6193e-01,\n",
       "          -1.7074e-01, -4.1851e-01,  2.0259e-01,  3.7172e-02, -7.3529e-02,\n",
       "           9.6385e-02, -5.4761e-02,  7.7320e-02, -1.8216e-01,  5.9645e-02,\n",
       "          -2.9964e-01, -2.4128e-01, -1.4941e-01,  1.3614e-01, -1.0828e-01,\n",
       "           1.3096e-01,  4.3867e-01, -1.5173e-01,  5.9410e-01, -3.6237e-01,\n",
       "           2.3492e-01, -1.5030e-01, -1.6252e-01]],\n",
       "\n",
       "        [[        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan],\n",
       "         [        nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan,         nan,         nan,\n",
       "                  nan,         nan,         nan]],\n",
       "\n",
       "        [[-1.2433e-01,  1.9002e-01, -2.1133e-01, -3.5906e-01,  4.2287e-01,\n",
       "           2.1797e-01, -1.5472e-01,  2.8877e-01,  1.5149e-01,  3.6336e-03,\n",
       "           1.0745e-01, -1.0742e-01,  9.6567e-02,  5.9231e-03, -5.8112e-03,\n",
       "          -2.1768e-01, -1.4614e-02,  3.1773e-01,  1.4917e-02,  1.3642e-01,\n",
       "           1.9300e-01, -8.2837e-02,  4.6095e-02,  2.1796e-02, -1.2993e-01,\n",
       "           2.8823e-02,  1.7655e-01, -1.2101e-01, -1.1085e-01,  8.1562e-02,\n",
       "           2.1754e-01, -1.4333e-01, -3.9137e-02, -2.3006e-01, -3.3018e-01,\n",
       "          -2.9920e-01,  1.9507e-01, -7.1842e-03,  7.4227e-02, -2.0992e-01,\n",
       "          -5.6588e-02, -1.3963e-01,  8.5904e-03,  1.3687e-01,  4.4889e-01,\n",
       "           1.3330e-01, -3.0268e-01,  1.3331e-01, -4.5488e-01, -3.3042e-02,\n",
       "           6.7202e-02, -4.3454e-02, -3.1545e-01, -1.0884e-01, -2.6332e-01,\n",
       "          -2.2818e-01, -6.8001e-02, -2.3287e-01,  1.1129e-01, -6.1920e-02,\n",
       "           2.5579e-01,  1.1905e-01, -4.0931e-01, -3.3080e-01,  4.2367e-02,\n",
       "          -1.4349e-01, -3.6352e-02, -2.1421e-02, -2.6395e-01, -8.4422e-02,\n",
       "          -2.7382e-01, -1.5996e-02,  4.8200e-01,  2.5816e-01, -2.4559e-01,\n",
       "          -1.1505e-01, -2.5910e-01, -2.2935e-01,  1.6483e-02,  1.3328e-01,\n",
       "          -3.7887e-01,  1.1474e-01, -1.0149e-01,  1.4343e-01,  1.2966e-01,\n",
       "           2.1860e-02,  8.9611e-03, -1.0721e-01,  2.0702e-02, -4.1403e-01,\n",
       "          -4.3463e-02, -1.0007e-01, -1.7264e-02, -4.0971e-01,  1.2829e-01,\n",
       "          -3.1941e-02,  1.9220e-01, -3.3952e-02,  2.6172e-01, -1.3839e-01,\n",
       "          -1.7148e-01, -2.4148e-01,  1.7765e-01,  1.6580e-01,  1.9454e-01,\n",
       "          -2.4469e-01, -2.6765e-01, -4.7425e-02,  5.3361e-02, -1.5627e-02,\n",
       "          -4.6547e-02,  3.7304e-02, -1.0778e-02, -1.6037e-01,  4.0096e-02,\n",
       "          -1.1868e-01, -3.0508e-01, -1.5678e-01,  1.2506e-01, -2.4042e-01,\n",
       "           4.9216e-02,  2.9380e-01, -2.8228e-01,  3.8354e-01, -2.0466e-01,\n",
       "           1.0309e-01,  2.2780e-02, -1.9158e-02],\n",
       "         [-2.2026e-01,  1.9741e-01, -2.7974e-01, -2.9605e-01,  4.2065e-01,\n",
       "           2.1558e-01, -1.2732e-01,  3.0244e-01,  1.9026e-01, -4.6898e-02,\n",
       "           1.3529e-01, -1.8427e-01,  1.0770e-01,  5.1373e-02,  4.8837e-03,\n",
       "          -1.5696e-01, -4.0933e-02,  4.0975e-01, -2.2884e-02,  1.7670e-01,\n",
       "           1.2737e-01, -8.9979e-02, -6.7379e-03,  7.8997e-02, -1.1502e-01,\n",
       "           9.0359e-02,  2.0489e-01, -1.4551e-01, -1.3062e-01,  9.3774e-02,\n",
       "           1.8636e-01, -1.2312e-01, -1.2724e-02, -1.8911e-01, -3.6055e-01,\n",
       "          -3.4177e-01,  1.7419e-01, -5.3019e-02,  1.1161e-01, -2.1190e-01,\n",
       "           1.8899e-03, -1.9890e-01,  5.2735e-02,  1.3113e-01,  4.3721e-01,\n",
       "           2.0070e-01, -3.5467e-01,  9.8499e-02, -4.3947e-01, -2.1193e-02,\n",
       "           4.1839e-02, -5.0321e-02, -3.2856e-01, -1.1368e-01, -2.7257e-01,\n",
       "          -2.4103e-01,  2.8213e-02, -2.7001e-01,  1.4391e-01, -1.1009e-01,\n",
       "           2.8961e-01,  9.7188e-02, -4.2410e-01, -3.3244e-01,  5.0701e-02,\n",
       "          -1.9176e-01, -1.2304e-01,  2.8083e-02, -2.7214e-01, -5.4467e-02,\n",
       "          -2.8242e-01,  7.4751e-02,  5.1754e-01,  2.1274e-01, -2.6631e-01,\n",
       "          -1.4642e-01, -2.7225e-01, -2.3950e-01,  6.2951e-02,  1.4031e-01,\n",
       "          -3.4450e-01,  6.0443e-02, -1.1076e-01,  1.7063e-01,  1.7492e-01,\n",
       "           3.5203e-02,  3.4963e-02, -9.7837e-02,  5.8625e-02, -4.2956e-01,\n",
       "           9.4251e-03, -8.6617e-02, -2.6617e-02, -4.5148e-01,  1.5516e-01,\n",
       "          -6.9893e-03,  1.7244e-01, -5.8363e-02,  2.6554e-01, -1.7889e-01,\n",
       "          -1.0518e-01, -1.8158e-01,  1.6071e-01,  1.3044e-01,  2.0699e-01,\n",
       "          -2.4748e-01, -2.6770e-01, -8.4542e-02,  4.2238e-02,  2.4610e-02,\n",
       "          -7.6261e-02, -3.9904e-03,  7.1187e-03, -1.4588e-01,  1.8951e-02,\n",
       "          -5.0255e-02, -2.0485e-01, -1.5871e-01,  1.3197e-01, -2.0276e-01,\n",
       "           5.9790e-02,  2.7935e-01, -3.7479e-01,  4.0284e-01, -2.1462e-01,\n",
       "           1.0572e-01, -2.9631e-02, -4.9762e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MutiheadTransformer(nn.Module):\n",
    "    def __init__(self,head_num,hidden_dim,drop_out = 0.1):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        \n",
    "        self.out_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,X,hidden_mask = None):\n",
    "        #   qkv (batch,s,h)  ->   (batch,head_num,s,head_dim)\n",
    "        batch , seq , _ =  X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        #   transform it's shape (batch,s,head_num,head_dim)  (batch,head_num,s,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        k = K.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        v = V.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)  \n",
    "\n",
    "        result_mid = (q @ k.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if hidden_mask is not None:\n",
    "            result_mid = result_mid.masked_fill(\n",
    "                hidden_mask == 0 , float('-inf') #   无限小的值\n",
    "            )\n",
    "        print(result_mid)\n",
    "\n",
    "        #   对行向量进行softmax\n",
    "        result_mid = torch.softmax(result_mid,dim=-1)\n",
    "        result_drop = self.drop_out(result_mid)\n",
    "\n",
    "        output_mid = (result_drop @ v).transpose(1,2).contiguous()\n",
    "        output_mid = output_mid.view(batch,seq,-1)#h\n",
    "\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "\n",
    "mask = torch.tensor([\n",
    "    [1,0],\n",
    "    [0,0],\n",
    "    [1,1]\n",
    "]).unsqueeze(1).unsqueeze(2).expand(3,8,2,2)\n",
    "print(mask.shape)\n",
    "X = torch.rand(3,2,128)\n",
    "model = MutiheadTransformer(8,128)# 8 head\n",
    "model(X,mask)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
