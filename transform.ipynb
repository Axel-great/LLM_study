{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from torch import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1647, -0.5922,  0.2470, -0.0232],\n",
       "         [-0.1650, -0.5883,  0.2439, -0.0200],\n",
       "         [-0.1641, -0.5926,  0.2476, -0.0244]],\n",
       "\n",
       "        [[-0.2515, -0.7480,  0.2592, -0.1423],\n",
       "         [-0.2503, -0.7461,  0.2571, -0.1425],\n",
       "         [-0.2504, -0.7468,  0.2562, -0.1411]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim:int = 728)->None:\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "    #初始化qkv\n",
    "        self.query_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.key_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.value_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        # Q : (batch_size,seq_len,dim)\n",
    "        # K*T:(batch_size,dim,seq_len)\n",
    "        Q = self.query_proj(X)\n",
    "        K = self.key_proj(X)\n",
    "        V = self.value_proj(X)\n",
    "\n",
    "        attention_value = torch.matmul(\n",
    "            #给k转置\n",
    "            Q,K.transpose(-1,-2)\n",
    "        )\n",
    "\n",
    "        attention_softmax = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim),dim=-1\n",
    "        )\n",
    "\n",
    "        result = torch.matmul(\n",
    "            attention_softmax,V\n",
    "        )\n",
    "        return result\n",
    "\n",
    "X = torch.rand(2,3,4)\n",
    "\n",
    "\n",
    "self_att_test = selfAttention(4)\n",
    "self_att_test(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4894, 0.5106],\n",
      "         [0.5109, 0.4891]],\n",
      "\n",
      "        [[0.5137, 0.4863],\n",
      "         [0.4978, 0.5022]],\n",
      "\n",
      "        [[0.5048, 0.4952],\n",
      "         [0.5059, 0.4941]]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4449,  0.2280, -0.5718,  0.0779],\n",
       "         [-0.4421,  0.2280, -0.5792,  0.0705]],\n",
       "\n",
       "        [[-0.5378,  0.1127, -0.5518,  0.1663],\n",
       "         [-0.5330,  0.1195, -0.5498,  0.1640]],\n",
       "\n",
       "        [[-0.5221,  0.0467, -0.6415,  0.1114],\n",
       "         [-0.5225,  0.0464, -0.6415,  0.1117]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class selfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.qkv = nn.Linear(dim,dim*3)\n",
    "\n",
    "    def forward(self,X):\n",
    "        # \n",
    "        QKV = self.qkv(X)\n",
    "        Q,K,V = torch.split(QKV,self.dim,dim=-1)\n",
    "\n",
    "        result = torch.softmax(torch.matmul(\n",
    "            Q,K.transpose(-1,-2)\n",
    "        ) / math.sqrt(self.dim),dim=-1\n",
    "        )\n",
    "        print(result)\n",
    "        output = result @ V\n",
    "        return output\n",
    "X = torch.rand(3,2,4)\n",
    "model2 = selfAttentionV2(4)\n",
    "model2(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiheadTransformer(nn.Module):\n",
    "    def __init__(self,head_num,hidden_dim,drop_out = 0.1):\n",
    "        super().__init__()\n",
    "        self.head_num = head_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        self.drop_out = nn.Dropout(drop_out)\n",
    "        \n",
    "        self.out_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "    def forward(self,X,hidden_mask = None):\n",
    "        #   qkv (batch,s,h)  ->   (batch,head_num,s,head_dim)\n",
    "        batch , seq , _ =  X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        #   transform it's shape (batch,s,head_num,head_dim)  (batch,head_num,s,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        k = K.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)   \n",
    "        v = V.view(batch,seq,self.head_num,self.head_dim).transpose(1,2)  \n",
    "\n",
    "        result_mid = (q @ k.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if hidden_mask is not None:\n",
    "            result_mid = result_mid.masked_fill(\n",
    "                hidden_mask == 0 , float('-inf') #   无限小的值\n",
    "            )\n",
    "        print(result_mid)\n",
    "\n",
    "        #   对行向量进行softmax\n",
    "        result_mid = torch.softmax(result_mid,dim=-1)\n",
    "        result_drop = self.drop_out(result_mid)\n",
    "\n",
    "        output_mid = (result_drop @ v).transpose(1,2).contiguous()\n",
    "        output_mid = output_mid.view(batch,seq,-1)#h\n",
    "\n",
    "        output = self.out_proj(output_mid)\n",
    "        return output\n",
    "\n",
    "mask = torch.tensor([\n",
    "    [1,0],\n",
    "    [0,0],\n",
    "    [1,1]\n",
    "]).unsqueeze(1).unsqueeze(2).expand(3,8,2,2)\n",
    "print(mask.shape)\n",
    "X = torch.rand(3,2,128)\n",
    "model = MutiheadTransformer(8,128)# 8 head\n",
    "model(X,mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDecoder(nn.Module):\n",
    "    def __init__(self,hidden_dim,head_num,dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.head_dim = hidden_dim // head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.q_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.k_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim,hidden_dim)\n",
    "\n",
    "        # upgrade dimension \n",
    "        self.up = nn.Linear(hidden_dim,hidden_dim*4)\n",
    "        self.down = nn.Linear(hidden_dim * 4 , hidden_dim)\n",
    "        self.act_proj = nn.ReLU()\n",
    "        # dropout\n",
    "        self.dropout_proj = nn.Dropout(dropout_rate)\n",
    "        self.layerNorm_proj = nn.LayerNorm(hidden_dim,eps=0.000001)\n",
    "\n",
    "\n",
    "    def attention_layer(self,q,k,v,mask,):\n",
    "        # (batch,seq,head_num,head_dim) -> (batch,head_num,seq,head_dim)\n",
    "        attention_mid = (q @ k.transpose(-1,-2)) / math.sqrt(self.head_dim)\n",
    "        # Itself tril and original mask\n",
    "        if mask is not None:\n",
    "            mask = mask.tril()\n",
    "            attention_mid = attention_mid.masked_fill(\n",
    "                mask == 0 , float('-inf')\n",
    "            )\n",
    "        else:\n",
    "            # make a ones trangles matrix which dimensions same like original \n",
    "            mask = attention_mid.ones_like().tril()\n",
    "            attention_mid = attention_mid.masked_fill(\n",
    "                mask == 0 , float('-inf')\n",
    "            )\n",
    "        # Specify a dimension row\n",
    "        attention_mid = torch.softmax(attention_mid,dim=-1)\n",
    "        # dropout layer\n",
    "        attention_dropout = self.dropout_proj(self.dropout_rate)\n",
    "        \n",
    "        #(batch,head_num,seq,head_dim)\n",
    "        output = (attention_dropout @ v)\n",
    "\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def ffn(self,X):\n",
    "        up = self.up(X)\n",
    "        up = self.act_proj(X)\n",
    "        down = self.down(up)\n",
    "\n",
    "        dropout = self.dropout_proj(down)\n",
    "\n",
    "        return self.layerNorm_proj(dropout + X)\n",
    "\n",
    "    def mha(self,X,mask):\n",
    "        batch,seq,_ = X.size()\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "        # (batch,seq,hidden_dim) -> (batch,seq,head_num,head_dim)\n",
    "        q = Q.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        k = K.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "        v = V.view(batch,seq,self.head_num,-1).transpose(1,2)\n",
    "\n",
    "        attention = self.attention_layer(q,k,v,mask).transpose(1,2).contiguous()\n",
    "        output = attention.view(batch,seq,-1)\n",
    "        return output\n",
    "\n",
    "    def forward(self,X,attention_mask=None):\n",
    "        X = self.mha(X,attention_mask)\n",
    "        X = self.ffn(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
